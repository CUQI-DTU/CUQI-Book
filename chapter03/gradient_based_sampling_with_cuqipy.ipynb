{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d1d192",
   "metadata": {},
   "source": [
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\dd{\\mathrm{d}}$\n",
    "\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\abs#1{\\left\\vert#1\\right\\vert}$\n",
    "\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\ve#1{\\bm{#1}}$\n",
    "</div>\n",
    "\n",
    "<div hidden>\n",
    "\n",
    "$\\gdef\\mat#1{\\mathbf{#1}}$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852081e",
   "metadata": {},
   "source": [
    "# Sampling with CUQIpy: five little stories\n",
    "\n",
    "## <font color=#CD853F> Contents of this notebook: </font>\n",
    "\n",
    "## <font color=#CD853F> Learning objectives: </font> <a name=\"r-learning-objectives\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154a666",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from cuqi.distribution import DistributionGallery, Gaussian, JointDistribution\n",
    "from cuqi.testproblem import Poisson1D\n",
    "from cuqi.problem import BayesianProblem\n",
    "import cuqi\n",
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cuqi.sampler import MH, CWMH, ULA, MALA, NUTS\n",
    "import time\n",
    "import scipy.stats as sps\n",
    "from scipy.stats import kde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8072f2",
   "metadata": {},
   "source": [
    "## <font color=#CD853F>  The two targets, again </font>\n",
    "\n",
    "Here we will add the code for building the two targets that we discussed in the previous [notebook](https://cuqi-dtu.github.io/CUQI-Book/chapter03/sampling_with_cuqipy_two_target.html). The details of the two targets can be found in that notebook. After running the following (hidden) code cell, we will have the two targets ready for sampling:\n",
    "- The donut target `target_donut` (distribution)\n",
    "- The Poisson 1D target `target_poisson` (posterior)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660b006",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# The donut distribution \n",
    "target_donut = DistributionGallery(\"donut\")\n",
    "\n",
    "# The Poisson1D Bayesian problem\n",
    "dim = 201\n",
    "L = np.pi\n",
    "\n",
    "xs = np.array([0.2, 0.4, 0.6, 0.8])*L\n",
    "ws = 0.8\n",
    "sigma_s = 0.05\n",
    "def f(t):\n",
    "    s = np.zeros(dim-1)\n",
    "    for i in range(4):\n",
    "        s += ws * sps.norm.pdf(t, loc=xs[i], scale=sigma_s)\n",
    "    return s\n",
    "\n",
    "A, _, _ = Poisson1D(dim=dim, \n",
    "                    endpoint=L,\n",
    "                    field_type='KL',\n",
    "                    field_params={'num_modes': 10} ,\n",
    "                    map=lambda x: np.exp(x), \n",
    "                    source=f).get_components()\n",
    "sigma_x = 30\n",
    "x = Gaussian(0, sigma_x**2, geometry=A.domain_geometry)\n",
    "np.random.seed(12)\n",
    "x_true = x.sample()\n",
    "sigma_y = np.sqrt(0.001)\n",
    "y = Gaussian(A(x), sigma_y**2, geometry=A.range_geometry)\n",
    "y_obs = y(x=x_true).sample()\n",
    "joint = JointDistribution(y, x)\n",
    "target_poisson = joint(y=y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec43e9",
   "metadata": {},
   "source": [
    "The in the following (hidden) code cell we again define the method `plot_pdf_1D` and `plot_pdf_2D` for plotting univariate and bi-variate pdfs. We have used these method before in [a previous notebook](https://cuqi-dtu.github.io/CUQI-Book/chapter01/Intro_example.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22432e49",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot2d(val, x1_min, x1_max, x2_min, x2_max, N2=201):\n",
    "    # plot\n",
    "    pixelwidth_x = (x1_max-x1_min)/(N2-1)\n",
    "    pixelwidth_y = (x2_max-x2_min)/(N2-1)\n",
    "\n",
    "    hp_x = 0.5*pixelwidth_x\n",
    "    hp_y = 0.5*pixelwidth_y\n",
    "\n",
    "    extent = (x1_min-hp_x, x1_max+hp_x, x2_min-hp_y, x2_max+hp_y)\n",
    "\n",
    "    plt.imshow(val, origin='lower', extent=extent)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "def plot_pdf_2D(distb, x1_min, x1_max, x2_min, x2_max, N2=201):\n",
    "    N2 = 201\n",
    "    ls1 = np.linspace(x1_min, x1_max, N2)\n",
    "    ls2 = np.linspace(x2_min, x2_max, N2)\n",
    "    grid1, grid2 = np.meshgrid(ls1, ls2)\n",
    "    distb_pdf = np.zeros((N2,N2))\n",
    "    for ii in range(N2):\n",
    "        for jj in range(N2):\n",
    "            distb_pdf[ii,jj] = np.exp(distb.logd(np.array([grid1[ii,jj], grid2[ii,jj]]))) \n",
    "    plot2d(distb_pdf, x1_min, x1_max, x2_min, x2_max, N2)\n",
    "\n",
    "def plot_pdf_1D(distb, min, max):\n",
    "    grid = np.linspace(min, max, 1000)\n",
    "    y = [distb.pdf(grid_point) for grid_point in grid]\n",
    "    plt.plot(grid, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28aaf92",
   "metadata": {},
   "source": [
    "## <font color=#CD853F>  Sampling the posterior </font>\n",
    "\n",
    "\n",
    "After defining the posterior distribution (or a target distribution in general) for the parameters of interest $\\mathbf{x}$, we can characterize the parameter and its uncertainty by samples from the posterior (target) distribution. However, in general the posterior (target) is not a simple distribution that we can easily sample from. Instead, we need to rely on Markov Chain Monte Carlo (MCMC) methods to sample from the posterior.\n",
    "\n",
    "In CUQIpy, a number of MCMC samplers are provided in the sampler module that can be used to sample probability distributions. All samplers have the same signature, namely `Sampler(target, ...)`, where `target` is the target CUQIpy distribution and `...` indicates any (optional) arguments.\n",
    "\n",
    "In CUQIpy we have a number of samplers available, that we can list using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb08b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "[sampler for sampler in dir(cuqi.sampler) if not sampler.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7506b775",
   "metadata": {},
   "source": [
    "Note that there is also the experimental `mcmc` module that has a new implementation of the samplers. This module is still under development and is aim to replace the current `sampler` module. We can list the samplers in the `mcmc` module using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7484b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[samplerNew for samplerNew in dir(cuqi.experimental.mcmc) if not samplerNew.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6d506",
   "metadata": {},
   "source": [
    "## <font color=#CD853F> Story 1 - It pays off to adapt: Comparing Metropolis Hastings (MH) with and without adaptation </font>\n",
    "\n",
    "- In the class, you learned about MH method.\n",
    "- In MH, for each iteration, a new sample is proposed based on the current sample (essintially adding a random perturbation to the corrent sample).\n",
    "- Then the porposed sample is accepted or rejected based on the acceptance probability: $\\min\\left(1, \\frac{p(\\mathbf{x}_{k+1})q(\\mathbf{x}_k|\\mathbf{x}_{k+1})}{p(\\mathbf{x}_k)q(\\mathbf{x}_{k+1}|\\mathbf{x}_k)}\\right)$, where $p(\\mathbf{x})$ is the target distribution and $q(\\mathbf{x}|\\mathbf{y})$ is the proposal distribution.\n",
    "- Here we test the MH method with and without adaptation.\n",
    "- For adaptation, MH uses vanishing adaptation to set the step size (scale) to achieve a desired acceptance rate (0.234 in this case).\n",
    "\n",
    "Let us sample the `target_donut` with a fixed `scale`. We first set up the number of samples, number of burn-in samples, and the fixed scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990991b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = 10000 # Number of samples\n",
    "Nb = 0 # Number of burn-in samples\n",
    "scale = 0.05 # Fixed step size \"scale\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9048607",
   "metadata": {},
   "source": [
    "We create the `MH` sampler with the target distribution `target_donut`, the scale `scale` and an initial sample `x0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_sampler = MH(target_donut, scale=scale, x0=np.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192db14",
   "metadata": {},
   "source": [
    "We sample using `sample` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf6824",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "MH_fixed_samples = MH_sampler.sample(Ns, Nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d04c0de",
   "metadata": {},
   "source": [
    "The samples are stored in a `Samples` object that is returned by the `sample` method. Let us visualize the samples using the `plot_pair` method, along with the density of the target distribution using the `plot_pdf_2D` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf_2D(target_donut, -4, 4, -4, 4)\n",
    "MH_fixed_samples.plot_pair(ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c1e4a",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- The samples captures only part of the target distribution.\n",
    "- Once the chain reaches the high density region, it stays there.\n",
    "- The samples starts at the center and takes some time to move to the donut shape. This part of the chain is the burn-in phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60bed7",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise: </font>\n",
    "- In CUQIpy we can remove the burin in in two ways. The first approach is to set Nb to the number of samples you want to remove. The second approach is after you already have the samples, you can use the `Samples` object `burnthin` method which removes the burn-in and applies thinning if the user asked for it. Try the second approach to remove a burn-in of size 1500 samples and plot the samples again (same as above, visualize both the density using `plot_pdf_2D` and the samples using the `plot_pair` method).\n",
    "- Try sampling with another scale (much smaller 0.005 and much larger 1) and visualize the samples, how does that affect the samples and the acceptance rate, and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5827e",
   "metadata": {},
   "source": [
    "Now we try the solving the same problem with adaptation. We make sure to set the scale to the previous value 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaec45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_sampler.scale = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5432617a",
   "metadata": {},
   "source": [
    "Then we sample this time using `sample_adapt` method which adapts the step size. Note that the adaptation happen during the burin-in phase so we need to set `Nb` to some large value to see the effect of adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c1a6f",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "Ns = 8500\n",
    "Nb = 1500\n",
    "MH_adapted_samples = MH_sampler.sample_adapt(Ns, Nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e3e26",
   "metadata": {},
   "source": [
    "Here we visualize the results (both the ones with and without adaptation). We set the color of the samples and the transparency `alpha` using the arviz argument `scatter_kwargs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf_2D(target_donut, -4, 4, -4, 4)\n",
    "MH_fixed_samples.plot_pair(ax=plt.gca(),scatter_kwargs={'c':'b', 'alpha':0.3})\n",
    "MH_adapted_samples.plot_pair(ax=plt.gca(),scatter_kwargs={'c':'r', 'alpha':0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa15304",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise: </font>\n",
    "- What do you notice about the acceptance rate and the scale in the case with adaptation compared to the previous case. \n",
    "- Time the two sampling methods, which one is faster and by what factor?\n",
    "- Compute the effective sample size for both cases. Note that you can use `compute_ess` method of the `Samples` object to compute the effective sample size. Which one is larger.\n",
    "- Scale the ESS by the time so that you have the ESS per second. Which one is larger? Does it pay off to use adaptation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7658f1",
   "metadata": {},
   "source": [
    "Let us plot the chains for the two cases, removing the burin-in from the fixed scale case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_fixed_samples.burnthin(Nb).plot_trace()\n",
    "MH_adapted_samples.plot_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2809b101",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "- The chains in the second case (with adaptation) has much better mixing compared to the first case (closer to a fuzzy warm shape).\n",
    "- The estimate density of the target distribution is much better in the second case, captures the 2 modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c0a77",
   "metadata": {},
   "source": [
    "We can also use `plot_pair` with additional arviz arguments `kind`, `kde_kwargs`, and `marginals` to visualize the marginals and the kernel density estimate of the target distribution. Let us do this for the two cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d837fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_adapted_samples.plot_pair(kind=[\"scatter\", \"kde\"], kde_kwargs={\"fill_last\": False}, marginals=True)\n",
    "MH_fixed_samples.plot_pair(kind=[\"scatter\", \"kde\"], kde_kwargs={\"fill_last\": False}, marginals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dea07b",
   "metadata": {},
   "source": [
    "It is clear from these figures how using adaptation of the step size is very beneficial to capture the target distribution in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7c582",
   "metadata": {},
   "source": [
    "##  <font color=#CD853F> Story 2 - Not all unkowns are created equal: Component-wise Metropolis Hastings CWMH </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba88b9",
   "metadata": {},
   "source": [
    "To get to the next sample, CWMH updates the components of the current sample one at a time and applies the accept/reject step for each component. Updating the scale happens for each component separately depending on the acceptance rate of the component.\n",
    "\n",
    "Let us test sampling the Poisson problem target with CWMH. We fix the seed for reproducibility and set the number of samples, burn-in samples, and the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8734156",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # Fix the seed for reproducibility\n",
    "scale = 0.06\n",
    "Ns = 500\n",
    "Nb = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35174b07",
   "metadata": {},
   "source": [
    "For comparison, we sample the Poisson problem target with the MH sampler first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3260d",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "MH_sampler = MH(target_poisson, scale = scale, x0=np.ones(target_poisson.dim))\n",
    "MH_samples = MH_sampler.sample_adapt(Ns, Nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e800053",
   "metadata": {},
   "source": [
    "Then we create a `CWMH` sampler with the target distribution `target_poisson`, the scale `scale` and an initial sample `x0` and sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e8e3a",
   "metadata": {
    "tags": [
     "scroll-outupt"
    ]
   },
   "outputs": [],
   "source": [
    "CWMH_sampler = CWMH(target_poisson, scale = scale, x0=np.ones(target_poisson.dim))\n",
    "CWMH_samples = CWMH_sampler.sample_adapt(Ns, Nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4031b338",
   "metadata": {},
   "source": [
    "Note that we use adaptation in both cases. Let us visualize the credible intervals in both cases: for `MH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_samples.plot_ci(exact=x_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d3b73",
   "metadata": {},
   "source": [
    "And for `CWMH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWMH_samples.plot_ci(exact=x_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32dd531",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise: </font>\n",
    "- We mentioned in the previous [notebook](https://cuqi-dtu.github.io/CUQI-Book/chapter03/sampling_with_cuqipy_two_target.html) that the credible interval (CI) visualized here is computed on the KL coefficients and then mapped to the function values (through computing the linear combination of the KL basis weighted by the KL coefficients and applying the map $x \\rightarrow e^x $). We are interested in mapping all the samples to the function values and then computing the credible interval. To achieve this in CUQIpy, one can use the property `funvals` of the `Samples` object. This property returns the function values of the samples wrapped in another `Samples` object. Use this property to create the `Samples` objects of function values `MH_samples_funvals` and `CWMH_samples_funvals` and then compute the credible interval on these new `Samples` object. Do this for both cases, the `MH` and the `CWMH` cases. Which sampler resulted in samples with a better credible interval?\n",
    "\n",
    "- Which approach of visualizing the credible interval do you prefer (mapping then computing the CI or the other way around)? Why?\n",
    "\n",
    "- Visualize the credible interval for both cases in the coefficient space. To achieve this in CUQIpy, pass the argument `plot_par=True` to the `plot_ci` method. What do you notice? what do you observe about the credible intervals in the coefficient space as the coefficient index increases? Hint: use the original `MH_samples` and `CWMH_samples` objects and not the ones converted to function values. Which sampler performs better in the coefficient space?\n",
    "\n",
    "\n",
    "- Plot the effictive sample size for both cases in the same plot (where the $x-\\text{axis}$ is the sample index and the $y-\\text{axis}$ is the effective sample size). What do you notice? Hint: use the `compute_ess` method of the `Samples` object to compute the effective sample size.\n",
    "\n",
    "- Plot the sampler scale for both cases in the same plot (the $x-\\text{axis}$ is the sample index and the $y-\\text{axis}$ is the scale). What do you notice? Hint: you can use `CWMH.scale` and `MH.scale` to get the scale for each sampler. Keep in mind that the scale for the `MH` sampler is the same for all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b92d8",
   "metadata": {},
   "source": [
    "##  <font color=#CD853F> Story 3 -  May the force guide you: Unadjusted Langevin algorithm (ULA)  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695a432",
   "metadata": {},
   "source": [
    "* In the Unadjusted Langevin algorithm (ULA), new states are proposed using (overdamped) Langevin dynamics.\n",
    "\n",
    "* Let $p$ denote a probability density on $\\mathbb{R}^d$, from which it is desired to draw an ensemble of i.i.d. samples. We consider the overdamped Langevin Itô diffusion\n",
    " \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\dfrac{\\mathrm{d}\\boldsymbol{x}_t}{\\mathrm{d} t} = \\dfrac{1}{2}\\nabla\\log\\pi(\\boldsymbol{x}) + \\dfrac{\\mathrm{d} W_t}{\\mathrm{d} t},\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "driven by the time derivative of a standard Brownian motion $W$ and force given as gradient of a potential $\\log\\pi(\\boldsymbol{x})$.\n",
    "\n",
    "* In the limit, as $t\\to\\infty$, the probability distribution of $X(t)$ approaches a stationary distribution, which is also invariant under the diffusion. It turns out that this distribution is $p$.\n",
    " \n",
    "* Approximate sample paths of the Langevin diffusion can be generated by the Euler--Maruyama method with a fixed time step $\\varepsilon>0$. We set $\\boldsymbol{x}_{0}$ and then recursively define an approximation to the true solution by\n",
    " \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\boldsymbol{x}_{k+1} := \\boldsymbol{x}_{k} + \\dfrac{\\varepsilon^2}{2} \\nabla \\log p(\\boldsymbol{x}_{k})+ \\varepsilon\\,\\boldsymbol{\\xi}_{k},\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where each $\\boldsymbol{\\xi}_k\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I}_d)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fa56a",
   "metadata": {},
   "source": [
    "Let us create the ULA sampler in CUQIpy with the target distribution `target_donuts`, a fixed scale `scale`, and an initial sample `x0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320260ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ULA_sampler = ULA(target=target_donut, scale=0.065, x0=np.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa7e6c",
   "metadata": {},
   "source": [
    "We set the number of samples, and sample the target distribution using the `sample` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb129f2",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "Ns = 1000\n",
    "ULA_samples = ULA_sampler.sample(Ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2c06e",
   "metadata": {},
   "source": [
    "And visualize the samples using the `plot_pair` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf_2D(target_donut, -4, 4, -4, 4)\n",
    "ULA_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'alpha': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bf482",
   "metadata": {},
   "source": [
    "We note that the samples are all over the domain and do not capture the target distribution. This is because the ULA sampler will result in approximate (biased) distribution that as $\\varepsilon \\rightarrow 0$ will converge to $p(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a74d2a1",
   "metadata": {},
   "source": [
    "##### <font color=#8B4513> Exercise: </font>\n",
    "- Try a different scales 0.01 and visualize the samples in the same way as above. What do you notice about the samples? does it capture the target distribution?\n",
    "- Using the new scale 0.01, sample only 10 samples. Using this scale as well, generate 10 MH samples using a MH sampler `MH_10 = MH(target_donut, scale=0.01, x0=np.array([0, 0]))`. Visualize the samples of both samplers using the `plot_pair` method (along with the pdf plot `plot_pdf_2D(target_donut, -4, 4, -4, 4)`). What do you notice about the samples of the two samplers in terms of finding the high density region of the target distribution?\n",
    "- Try ULA for a univariate target distribution `x_uni = Gaussian(0, 1)`:\n",
    "    - Create an ULA sampler object `ULA_uni` for the target distribution `x_uni` with scale 1 and initial sample 0.\n",
    "    - Generate 40000 samples and store it in `ula_samples_uni`.\n",
    "    - Compare the kernel density estimation (KDE) of the samples with the target distribution `x_uni` PDF. What do you notice? does the KDE capture the target distribution? Hint: to plot the PDE use the `plot_pdf_1D` method and to plot the KDE generate a KDE using scipy.stats kde (already imported here) `kde_est = kde.gaussian_kde(ula_samples_uni.samples[0,:])`, then create a grid `x = np.linspace(-4, 4, 1000)` and plot the KDE using `plt.plot(x, kde_est(x))`.\n",
    "    - Try a different scale 0.1 and repeat the above steps. What do you notice about the samples KDE? does it capture the target distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea6a8e6",
   "metadata": {},
   "source": [
    "##  <font color=#CD853F> Story 4: This is a bias we can fix - Metropolis-adjusted Langevin algorithm (MALA) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2baae55",
   "metadata": {},
   "source": [
    "* In contrast to the Euler--Maruyama method for simulating the Langevin diffusion. MALA incorporates an additional step. We consider the above update rule as defining a proposal $\\tilde{\\boldsymbol{x}}$ for a new state:\n",
    " \n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\tilde{\\boldsymbol{x}}}_{k+1}:=\\boldsymbol{x}_{k}+\\dfrac{\\varepsilon^2}{2} \\nabla \\log p(\\boldsymbol{x}_{k})+ \\varepsilon\\,\\boldsymbol{\\xi}_{k};\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "this proposal is accepted or rejected according to the MH algorithm.\n",
    "\n",
    "* That is, the acceptance probability is:\n",
    " \n",
    " $$\n",
    "\\begin{equation}\n",
    "\\alpha(\\boldsymbol{x}_k,{\\tilde{\\boldsymbol{x}}}_{k+1}) = \\min\\left(1,\\dfrac{p({\\tilde{\\boldsymbol{x}}}_{k+1})q(\\boldsymbol{x}_k \\,\\vert\\, {\\tilde{\\boldsymbol{x}}}_{k+1})} {p(\\boldsymbol{x}_k)q({\\tilde{\\boldsymbol{x}}}_{k+1} \\,\\vert\\, \\boldsymbol{x}_k)}\\right);\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where the proposal has the form\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "q(\\boldsymbol{x}' \\,\\vert\\, \\boldsymbol{x}) = \\mathcal{N}\\left(\\boldsymbol{x}';\\, \\boldsymbol{x}+\\dfrac{\\varepsilon^2}{2} \\nabla \\log p(\\boldsymbol{x}),\\varepsilon^2\\boldsymbol{I}_d\\right).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* The combined dynamics of the Langevin diffusion and the MH algorithm satisfy the detailed balance conditions necessary for the existence of a unique, invariant, stationary distribution.\n",
    "\n",
    "* For limited classes of target distributions, the optimal acceptance rate for this algorithm can be shown to be $0.574$; this can be used to tune $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqi.distribution import Gaussian\n",
    "from cuqi.sampler import MALA\n",
    "uni_x = Gaussian(0,1)\n",
    "MALA_2 = MALA(target=uni_x, scale=1, x0=np.array([0]))\n",
    "Ns = 40000\n",
    "ula_samples = MALA_2.sample(Ns)\n",
    "#ula_samples.hist_chain(0,bins=100)\n",
    "from scipy.stats import kde\n",
    "kde_est = kde.gaussian_kde(ula_samples.samples[0,:])\n",
    "grid = np.linspace(-4, 4, 1000)\n",
    "plt.plot(grid, kde_est(grid))\n",
    "plot_pdf_1D(uni_x, -4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20c679",
   "metadata": {},
   "source": [
    "## <font color=#CD853F> Story 5 - Let us go nuts with NUTS: Hamiltonian Monte Carlo (HMC) and No U-turn sampler (NUTS) </font>\n",
    "\n",
    "\n",
    "* Hamiltonian Monte Carlo (HMC): use Hamiltonian dynamics to simulate particle trajectories.\n",
    " \n",
    "* Define a Hamiltonian function in terms of the target distribution.\n",
    " \n",
    "* Introduce an auxiliary momentum variables, which typically have independent Gaussian distributions.\n",
    " \n",
    "* Hamiltonian dynamics operate on a $d$-dimensional position vector $\\boldsymbol{x}$, and a $d$-dimensional momentum vector $\\boldsymbol{r}$, so that the full state space has $2d$ dimensions. The system is described by a function of $\\boldsymbol{x}$ and $\\boldsymbol{r}$ known as the Hamiltonian $H(\\boldsymbol{x}, \\boldsymbol{r})$.\n",
    "\n",
    "* In HMC, one uses Hamiltonian functions that can be written as (closed-system dynamics):\n",
    " \n",
    "$$\n",
    "\\begin{equation}\n",
    "H(\\boldsymbol{x}, \\boldsymbol{r}) = \\underbrace{U(\\boldsymbol{x})}_{\\text{potential energy}} + \\underbrace{K(\\boldsymbol{r},\\boldsymbol{x})}_{\\text{kinetic energy}}.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* The potential energy is completely determined by the target distribution, indeed $U(\\boldsymbol{x})$ is equal to the logarithm of the target distribution $p$.\n",
    "\n",
    "* The kinetic energy is unconstrained and must be specified by the implementation.\n",
    " \n",
    "* The Hamiltonian is an energy function for the joint state of position-momentum, and so defines a joint distribution for them as follows:\n",
    " \n",
    "$$\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol{x}, \\boldsymbol{r}) = \\dfrac{1}{Z}\\exp\\left(-\\dfrac{H(\\boldsymbol{x}, \\boldsymbol{r})}{T}\\right) = \\dfrac{1}{Z}\\exp\\left(-{U(\\boldsymbol{x})}\\right)\\exp\\left(-{K(\\boldsymbol{r})}\\right).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* There are several ways to set the kinetic energy (density of the auxiliary momentum):\n",
    "  - Euclidean--Gaussian kinetic energy: using a fixed covariance $\\boldsymbol{M}$ estimated from the position parameters, $K(\\boldsymbol{p},\\boldsymbol{q}) = \\dfrac{1}{2} \\boldsymbol{r}^T\\boldsymbol{M}^{-1}\\boldsymbol{r} + \\log( \\abs{\\boldsymbol{M}}) + \\text{const}$.\n",
    "  - Riemann--Gaussian kinetic energy: unlike the Euclidean metric, varies as one moves through parameter space, $K(\\boldsymbol{r}, \\boldsymbol{x}) = \\dfrac{1}{2} \\boldsymbol{r}^T\\boldsymbol{\\Sigma}(\\boldsymbol{x})^{-1}\\boldsymbol{r} + \\dfrac{1}{2}\\log( \\abs{\\boldsymbol{\\Sigma}(\\boldsymbol{x})}) + \\text{const}$.\n",
    "  - Non-Gaussian kinetic energies.\n",
    "  \n",
    "* Hamilton's equations read as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{\\mathrm{d} \\boldsymbol{x}}{\\dd t} &= +\\frac{\\partial H}{\\partial \\boldsymbol{r}} = \\boldsymbol{M}^{-1}\\boldsymbol{r} \\\\\n",
    "\\dfrac{\\mathrm{d} \\boldsymbol{r}}{\\mathrm{d} t} &= -\\dfrac{\\partial H}{\\partial \\boldsymbol{x}} = -\\dfrac{\\partial K}{\\partial \\boldsymbol{x}} - \\dfrac{\\partial U}{\\partial \\boldsymbol{x}},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\dfrac{\\partial U}{\\partial \\boldsymbol{x}}$ is the gradient of the logarithm of the target density.\n",
    "\n",
    "* Discretizing Hamilton's equations:\n",
    "  - Euler's method (no!).\n",
    "  - Modified Euler's method (a bit better!).\n",
    "  - Symplectic integrators: the leapfrog method (the standard choice!).\n",
    "  \n",
    "* Hamiltonian dynamics are time-reversible and volume-preserving.\n",
    "\n",
    "* The dynamics keep the Hamiltonian invariant. A Hamiltonian trajectory will (if simulated exactly) move within a hypersurface of constant probability density.\n",
    "\n",
    "* Each iteration of the HMC algorithm has two steps. Both steps leave the joint distribution of $p(\\boldsymbol{x}, \\boldsymbol{r})$ invariant (detailed balance).\n",
    "  - In the first step, new values of $\\boldsymbol{r}$ are randomly drawn from their Gaussian distribution, independently of the current $\\boldsymbol{x}$.\n",
    "  - In the second step, a Metropolis update is performed, using Hamiltonian dynamics to propose a new state.\n",
    "  - Optimal acceptance rate is 0.65. The step size $\\varepsilon$ and trajectory length $L$ need to be tuned.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e289c9",
   "metadata": {},
   "source": [
    "Code the HMC algorithm for the donut example.\n",
    "\n",
    "write code for HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724833b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cf990a2",
   "metadata": {},
   "source": [
    "\n",
    "## No-U-Turn sampler <a class=\"anchor\" id=\"NUTS\"></a>\n",
    "\n",
    "* HMC is an algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information.\n",
    "\n",
    "* However, HMC's performance is highly sensitive to two user-specified parameters: a step size $\\varepsilon$ and a desired number of steps $L$.\n",
    "\n",
    "*  The No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps $L$, as well as the step-size.\n",
    "\n",
    "* We simulate in discrete time steps, and to make sure you explore the parameter space properly you simulate steps in one direction and the twice as many in the other direction, turn around again, etc. At some point you want to stop this and a good way of doing that is when you have done a U-turn (i.e., appear to have gone all over the place).\n",
    "\n",
    "* NUTS begins by introducing an auxiliary variable with conditional distribution. After re-sampling from this distribution, NUTS uses the leapfrog integrator to trace out a path forwards and backwards in fictitious time. First running forwards or backwards 1 step, then forwards or backwards 2 steps, then forwards or backwards 4 steps, etc.\n",
    "\n",
    "* This doubling process implicitly builds a balanced binary tree whose leaf nodes correspond to position-momentum state. The doubling is stopped when the subtrajectory from the leftmost to the rightmost nodes of any balanced subtree of the overall binary tree starts to double back on itself (i.e., the fictional particle starts to make a U-turn).\n",
    "\n",
    "* At this point NUTS stops the simulation and samples from among the set of points computed during the simulation, taking care to preserve detailed balance.\n",
    "\n",
    "* To adapt the step-size, NUTS uses a modified dual averaging algorithm during the burn-in phase.\n",
    "\n",
    "* The good thing about NUTS is that proposals are made based on the shape of the posterior and they can happend at the other end of the distribution. In contrast, MH makes proposals within a ball, and Gibbs sampling only moves along one (or at least very few) dimensions at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqi.sampler import NUTS\n",
    "NUTS_sampler = NUTS(target=target, x0=np.array([0,0]))\n",
    "\n",
    "# now let us sample\n",
    "Ns = 10#40000\n",
    "Nb = 10\n",
    "NUTS_samples = NUTS_sampler.sample(Ns, Nb)\n",
    "\n",
    "\n",
    "plot_pdf_2D(target, -4, 4, -4, 4)\n",
    "NUTS_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'alpha': 0.5, 'c': 'r', 'edgecolors':'r'})\n",
    "plt.plot(NUTS_samples.samples[0,:], NUTS_samples.samples[1,:], 'y', alpha=0.5)\n",
    "\n",
    "plt.plot(NUTS_sampler._idx, NUTS_sampler._tree )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect the lines in scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c85787",
   "metadata": {},
   "outputs": [],
   "source": [
    "Poisson1D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02064053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson inverse problem\n",
    "from cuqi.testproblem import Poisson1D\n",
    "from cuqi.distribution import GMRF, Gaussian, CMRF\n",
    "from cuqi.problem import BayesianProblem\n",
    "from cuqi.sampler import MH, CWMH, pCN\n",
    "\n",
    "# Load forward model, data and problem information\n",
    "n = 20\n",
    "Ns = 100\n",
    "Nb = 30\n",
    "cov = 4.62905925e-05\n",
    "A, y_data, probInfo = Poisson1D(dim=n, field_type='KL', map=lambda x:np.exp(x)).get_components()\n",
    "\n",
    "print(A.domain_geometry)\n",
    "\n",
    "x = Gaussian(np.zeros(A.domain_dim), 1, geometry=A.domain_geometry)\n",
    "#x = GMRF(np.zeros(A.domain_dim), 8, geometry=A.domain_geometry,  order=1)\n",
    "y = Gaussian(A(x), cov)\n",
    "\n",
    "BP = BayesianProblem(x, y).set_data(y=y_data)\n",
    "posterior = BP.posterior()\n",
    "\n",
    "\n",
    "posterior.enable_FD()\n",
    "\n",
    "NUTS_sampler = NUTS(posterior, x0=np.ones(n), max_depth=7)\n",
    "NUTS_samples = NUTS_sampler.sample(Ns, Nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTS_samples.funvals.plot_ci(95, exact=probInfo.exactSolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ddbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Poisson1D(dim=n, field_type='KL', map=lambda x:np.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.likelihood.distribution.cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca92f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "probInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "probInfo.infoString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "#\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "import cuqi\n",
    "\n",
    "# =======================================================\n",
    "# domain discretization\n",
    "N = 201\n",
    "L = np.pi\n",
    "t = np.linspace(0, L, N)\n",
    "\n",
    "# =======================================================\n",
    "# input: piece-wise constant random function\n",
    "conds = [(0 <= t) & (t < L/4), (L/4 <= t) & (t < L/2), \\\n",
    "        (L/2 <= t) & (t < 3*L/4), (3*L/4 <= t) & (t <= L)]\n",
    "d = len(conds)\n",
    "U = lambda theta: np.piecewise(t, conds, theta) \n",
    "X = lambda u: np.exp(u)\n",
    "\n",
    "# random parameters\n",
    "mu_theta = -1           # mean of the underlying Gaussian\n",
    "sigma_theta = 1         # std of the underlying Gaussian\n",
    "theta_s = lambda n: np.random.normal(mu_theta, sigma_theta, size=(d, n))\n",
    "\n",
    "# =======================================================\n",
    "# define model\n",
    "# source term\n",
    "xs = np.array([0.2, 0.4, 0.6, 0.8])*L\n",
    "ws = 0.8\n",
    "sigma_s = 0.05\n",
    "def source(t):\n",
    "    s = np.zeros(N-1)\n",
    "    for i in range(4):\n",
    "        s += ws * sps.norm.pdf(t, loc=xs[i], scale=sigma_s)\n",
    "    return s\n",
    "\n",
    "# set model\n",
    "model, _, _ = cuqi.testproblem.Poisson1D(\n",
    "                                    dim=N, \n",
    "                                    endpoint=L,\n",
    "                                    field_type='KL',\n",
    "                                    field_params={'num_modes': 10} ,\n",
    "                                    map=lambda x: np.exp(x), \n",
    "                                    source=source).get_components()\n",
    "t_range = model.range_geometry.grid\n",
    "\n",
    "#x = GMRF(np.zeros(N), 9, geometry=model.domain_geometry,  order=1)\n",
    "np.random.seed(12)\n",
    "grid = model.domain_geometry.grid\n",
    "\n",
    "#x_true = cuqi.array.CUQIarray( 0.4*np.sin(grid)+ 1, geometry=model.domain_geometry)\n",
    "x = Gaussian(0, 900, geometry=model.domain_geometry)\n",
    "x_true = x.sample()\n",
    "y = Gaussian(model(x), 0.001, geometry=model.range_geometry)\n",
    "y_data = y(x=x_true).sample()\n",
    "\n",
    "y_data.plot()\n",
    "model(x_true).plot()\n",
    "\n",
    "x_true.plot()\n",
    "\n",
    "joint = cuqi.distribution.JointDistribution(x, y)\n",
    "posterior = joint(y=y_data)\n",
    "\n",
    "posterior.enable_FD()\n",
    "MH_sampler = NUTS(posterior, max_depth=7)#,  scale=0.1)\n",
    "Ns = 100#5000\n",
    "Nb = 20#2000\n",
    "MH_samples = MH_sampler.sample_adapt(Ns, Nb)\n",
    "MH_samples.compute_ess()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_samples.burnthin(int(Ns*0.1)).plot_ci(95, exact=x_true)\n",
    "plt.figure()\n",
    "MH_samples.funvals.burnthin(int(Ns*0.1)).plot_ci(95, exact=x_true)\n",
    "plt.figure()\n",
    "x.sample(5).plot()\n",
    "plt.figure()\n",
    "MH_samples.burnthin(int(Ns*0.1)).plot_ci(95, plot_par=True)\n",
    "\n",
    "print(MH_samples.compute_ess())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1453eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuqi.geometry.KLExpansion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad49607",
   "metadata": {},
   "source": [
    "increate number of modes\n",
    "what do you observe?\n",
    "try pcn (better ESS), \n",
    "pcn does acceptance rate change with number of modes?\n",
    "deconstruct te model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414eb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.enable_FD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35e832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7633b639",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786670b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9355f0b3",
   "metadata": {},
   "source": [
    "In both cases the chains look very good with no discernible difference between the start and end of the chain. This is a good indication that the chain has converged and there is little need for removing samples that are part of a \"burn-in\" period. In practice, the samples should be inspected with more rigor to ensure that the MCMC chain has converged, but this is outside the scope of this notebook.\n",
    "\n",
    "The good sampling is in large part due to the LinearRTO sampler, which is built specifically for the type of problem of this example. For the sake of presentation let us remove the first 100 samples using the `burnthin` method (see [samples.burnthin](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.samples/cuqi.samples.Samples.burnthin.html#cuqi.samples.Samples.burnthin)) and store the \"burnthinned\" samples in a new variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa612c6",
   "metadata": {},
   "source": [
    "- list of all samplers\n",
    "- burn thin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling with CUQIpy: Two target distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aee6d1",
   "metadata": {},
   "source": [
    "### Trying out other samples\n",
    "\n",
    "The LinearRTO sampler can only sample Gaussian posteriors that also have an underlying linear model.\n",
    "\n",
    "It is possible to try out other CUQIpy samplers (which also work for a broader range of problems). For example:\n",
    "\n",
    "* **[pCN](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.pCN.html#cuqi.sampler.pCN)** - preconditioned Crank-Nicolson sampler.\n",
    "* **[CWMH](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.CWMH.html)** - Component-wise Metropolis-Hastings sampler.\n",
    "* **[ULA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.ULA.html)** - Unadjusted Langevin Algorithm.\n",
    "* **[MALA](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.MALA.html)** - Metropolis Adjusted Langevin Algorithm.\n",
    "* **[NUTS](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.html)** - No U-Turn Sampler: A variant of the Hamiltonian Monte Carlo sampler well-established in literature.\n",
    "\n",
    "Note in particular that ULA, MALA and NUTS all require the gradient of the logpdf. This is handled automatically in CUQIpy for linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8544d48",
   "metadata": {},
   "source": [
    "#### ★ Try yourself (optional):  \n",
    "\n",
    "Try sampling the posterior above using one of the suggested samplers (click the links to look at the online documentation for the sampler to get more info on it).\n",
    "\n",
    "Compare results (chain, credibility interval etc.) to the results from LinearRTO.\n",
    "\n",
    "All the suggested samplers (except NUTS) will likely require > 5000 samples to give reasonable results, and for some playing with step sizes (scale) is needed. This is because they are not as efficient as LinearRTO or NUTS. For some samplers, the method [sample_adapt](https://cuqi-dtu.github.io/CUQIpy/api/_autosummary/cuqi.sampler/cuqi.sampler.NUTS.sample_adapt.html#cuqi.sampler.NUTS.sample_adapt) will auto-scale the step size according to some criteria, e.g. reach approximately optimal acceptance rate and a burn-in should be added to specify how many samples to use for the adaptation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792eff9",
   "metadata": {},
   "source": [
    "# 4. Computing point estimates of the posterior ★ <a class=\"anchor\" id=\"pointestimates\"></a>\n",
    "\n",
    "In addition to sampling the posterior, we can also compute point estimates of the posterior. A common point estimate to consider is the Maximum A Posteriori (MAP) estimate, which is the value of the Bayesian parameter that maximizes the posterior density. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_\\mathrm{MAP} = \\arg\\max_\\mathbf{x} p(\\mathbf{x} \\mid \\mathbf{y}_\\mathrm{data}).\n",
    "\\end{align*}\n",
    "\n",
    "The easiest way to compute the MAP estimate is to use the `MAP` method of the `BayesianProblem` class as follows:\n",
    "\n",
    "```python\n",
    "x_map.plot()\n",
    "plt.title('MAP estimate')\n",
    "plt.show()\n",
    "\n",
    "probInfo.exactSolution.plot()\n",
    "plt.title('Exact solution')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3572ff84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b1d1306",
   "metadata": {},
   "source": [
    "#CWMH_sampler = CWMH(target, scale=scale, x0=np.array([0,0]))\n",
    "#CWMH_fixed_samples = CWMH_sampler.sample(Ns, Nb)\n",
    "#print('Elapsed time CWMH:', time.time() - ti, '\\n')\n",
    "#CWMH_fixed_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'c':'g'})\n",
    "#\n",
    "#\n",
    "#CWMH_adapted_samples = CWMH_sampler.sample_adapt(Ns, Nb)\n",
    "#print('Elapsed time CWMH:', time.time() - ti, '\\n')\n",
    "#CWMH_adapted_samples.plot_pair(ax=plt.gca(), scatter_kwargs={'c':'y'})\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
