{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Under construction for prior.ipynb**\n",
    "- Point discretization\n",
    "- Series expansion KL / custom KL (dim reduction), something similar to paper II, understanding these parameterization\n",
    "- Distribution with largest entropy\n",
    "- ““uniquely determined as the one which is maximally noncommittal with regard to missing information, in that it agrees with what is known, but expresses maximum uncertainty with respect to all other matters”.”\n",
    "- Ex, given a grid, compute the entropy, insight on why we often use the Gaussian\n",
    "- Exercise about “Mixtures of conjugate priors are also conjugate”\n",
    "- This is the distribution, this is the likelihood, create a distribution that proves conjugacy \n",
    "- Conjugacy example, example 2, build posterior of two gaussian prior and likelihood, verify it is the same thing by sampling (slide 38)/ covariance update?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Under construction for Intro_example.ipynb**\n",
    "\n",
    "- \"The data cannot make possible an event that is impossible under the prior, quote, example maybe with uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 3:\n",
    "\n",
    "- Langavin algorithms \n",
    "- Math background, how this stems from discretizing a stochastic PDE, the bias\n",
    "- ULA\n",
    "- ULA algorithm\n",
    "- Use cuqipy to demonstrate the difference of MH and ULA\n",
    "- Use cuqipy to demonstrate the bias\n",
    "\n",
    "- MALA\n",
    "- MALA algorithm \n",
    "- Use cuqipy to demonstrate eliminating the bias\n",
    "- Exercise  using MALA, code the gradient yourself\n",
    "- Analyze the results \n",
    "\n",
    "\n",
    "**Hamiltonian algorithms**\n",
    "\n",
    "- NUTS\n",
    "- Math background, how this stems from discretizing a stochastic PDE\n",
    "- Some videos / demo (can do with call back as well)\n",
    "- NUTS algorithm \n",
    "- Analyze the results\n",
    "- Look at the variables tree size, \n",
    "\n",
    "- What is the effect of the initial point, problem on the tree size \n",
    "- can we have favored starting point, step size.\n",
    "\n",
    "- Solving heat with NUTS, or Poisson/Fenics with NUTS vs MH  \n",
    "- Verify gradient, compare gradient performance FD and adjoint (as increasing KL expansion order)\n",
    "- Link to gradient based optimization?\n",
    "\n",
    "\n",
    "#### Chapter 4:\n",
    "- FEniCS crash course\n",
    "- Adjoint based gradient (example: poisson)\n",
    "- DTO and OTD\n",
    "- (Can be done in FD?)\n",
    "\n",
    "- Free exploration CUQIpy resource then present \n",
    "- The notebook on Bayesian inversion \n",
    "- Gelman workflow \n",
    "- Can go over demos/examples/etc/ and get ideas from there "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
